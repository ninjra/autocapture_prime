"""Torch ColBERT reranker.

Prefers embeddings generated by `builtin.index.colbert_torch`, but falls back
to hash-index embeddings if needed.
"""

from __future__ import annotations

from typing import Any

from autocapture_nx.indexing.colbert import ColbertSQLiteStore, HashTokenEmbedder, default_colbert_db_path, maxsim_score
from autocapture_nx.plugin_system.api import PluginBase, PluginContext


class TorchColbertReranker(PluginBase):
    def __init__(self, plugin_id: str, context: PluginContext) -> None:
        super().__init__(plugin_id, context)
        cfg = context.config if isinstance(context.config, dict) else {}
        self._weight = float(cfg.get("weight") or 1.0)
        self._model_id = str(cfg.get("model_id") or "").strip()
        self._device = str(cfg.get("device") or "cuda").strip()
        self._fallback = HashTokenEmbedder(dim=int(cfg.get("dim") or 32))
        db_path = str(cfg.get("db_path") or "").strip()
        self._store = ColbertSQLiteStore(db_path or default_colbert_db_path())
        self._model = None
        self._tokenizer = None
        self._err: str | None = None
        self._try_load()

    def capabilities(self) -> dict[str, Any]:
        return {"retrieval.reranker": self}

    def _try_load(self) -> None:
        if not self._model_id:
            self._err = "model_id_missing"
            return
        try:
            import torch  # noqa: F401
            from transformers import AutoModel, AutoTokenizer

            tok = AutoTokenizer.from_pretrained(self._model_id, trust_remote_code=True)
            mdl = AutoModel.from_pretrained(self._model_id, trust_remote_code=True)
            mdl = mdl.to(self._device)
            mdl.eval()
            self._tokenizer = tok
            self._model = mdl
            self._err = None
        except Exception as exc:
            self._tokenizer = None
            self._model = None
            self._err = f"load_failed:{type(exc).__name__}"

    def _embed_query(self, text: str):  # type: ignore[no-untyped-def]
        if self._model is None or self._tokenizer is None:
            return None
        import numpy as np
        import torch

        toks = self._tokenizer(
            text,
            truncation=True,
            max_length=128,
            return_tensors="pt",
            add_special_tokens=True,
        )
        toks = {k: v.to(self._device) if hasattr(v, "to") else v for k, v in toks.items()}
        with torch.no_grad():
            out = self._model(**toks)
        last_hidden = getattr(out, "last_hidden_state", None)
        if last_hidden is None:
            return None
        vec = last_hidden[0].detach().float().cpu().numpy()
        norms = np.linalg.norm(vec, axis=1, keepdims=True)
        norms[norms == 0.0] = 1.0
        return (vec / norms).astype(np.float32)

    def rerank(self, query: str, docs: list[dict[str, Any]]) -> list[dict[str, Any]]:
        if not docs:
            return docs
        q = str(query or "").strip()
        if not q:
            return docs
        qmat = self._embed_query(q)
        if qmat is None:
            qmat = self._fallback.embed_query_tokens(q)
            ident = self._fallback.identity()
            digests = [str(ident.get("embedder_digest") or "")]
        else:
            from autocapture_nx.kernel.hashing import sha256_text

            digests = [sha256_text(f"{self._model_id}:{qmat.shape[1]}:{self._device}")[:16]]
            # Accept hash fallback docs too.
            digests.append(str(self._fallback.identity().get("embedder_digest") or ""))
        doc_ids = [str(d.get("doc_id") or d.get("record_id") or "") for d in docs]
        rows = {}
        for digest in digests:
            if not digest:
                continue
            part = self._store.get_docs(doc_ids=doc_ids, embedder_digest=digest)
            for k, v in part.items():
                rows.setdefault(k, v)

        scored: list[dict[str, Any]] = []
        for d in docs:
            did = str(d.get("doc_id") or d.get("record_id") or "")
            base = float(d.get("score", 0.0) or 0.0)
            row = rows.get(did)
            if row and row.get("blob") and int(row.get("dim") or 0) > 0:
                li = maxsim_score(qmat, row["blob"], dim=int(row.get("dim") or 0))
                score = base + (self._weight * float(li))
                scored.append({**d, "score": score, "rerank": {"colbert_torch": li, "error": self._err}})
            else:
                scored.append({**d, "score": base, "rerank": {"colbert_torch": None, "error": self._err}})
        scored.sort(key=lambda x: (-float(x.get("score", 0.0) or 0.0), str(x.get("doc_id") or "")))
        return scored


def create_plugin(plugin_id: str, context: PluginContext) -> TorchColbertReranker:
    return TorchColbertReranker(plugin_id, context)


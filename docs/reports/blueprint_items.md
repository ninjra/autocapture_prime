# Blueprint Items (I001–I130)

Source: `docs/blueprints/autocapture_nx_blueprint.md`

| ItemID | Phase | Title | Enforcement location (key) | Regression detection (key) | Acceptance criteria (key) |
| --- | --- | --- | --- | --- | --- |
| I084 | Phase 0: Scaffolding and gates | Split heavy ML dependencies into optional extras | pyproject.toml extras<br>plugin import boundaries (no unconditional torch/transformers imports)<br>builtin plugin manifests declare optional deps | CI matrix: minimal install runs capture+stores without ML deps<br>CI matrix: extras[vision]/[embeddings]/[sqlcipher] enable corresponding plugins<br>Gate: import-time scan ensures no heavy deps imported in ACTIVE ingest path | Base installation captures and queries metadata without importing heavy ML deps.<br>Installing extras enables the associated plugins with no code changes. |
| I085 | Phase 0: Scaffolding and gates | Make resource paths package-safe (no CWD dependence) | Replace relative paths with `importlib.resources` for packaged assets<br>`autocapture_nx/kernel/paths.py` (new) centralizes path resolution | Test: run from arbitrary CWD and verify default.json/contracts/plugins load<br>Wheel install test: builtin plugins discoverable and loadable | Running from any directory yields identical behavior and finds assets. |
| I086 | Phase 0: Scaffolding and gates | Use OS-appropriate default data/config dirs (platformdirs) | Use `platformdirs` to pick Windows-first locations<br>Config schema: `paths.config_dir`, `paths.data_dir` resolved at boot | Test matrix: Windows/Linux/WSL path resolution produces valid dirs<br>Doctor check: directories exist and are writable; vault is restricted | Default paths are correct on Windows and do not depend on CWD. |
| I087 | Phase 0: Scaffolding and gates | Package builtin plugins as package data | Packaging: include `plugins/builtin/**` as package data<br>Plugin discovery uses package resources when installed | Wheel install test: `autocapture doctor` lists builtin plugins<br>Gate: plugin lock hashing includes packaged plugin files | Installed wheel runs with builtin plugins without requiring repo checkout. |
| I088 | Phase 0: Scaffolding and gates | Add reproducible dependency lockfile (hash-locked) | Dependency lock: `requirements.lock` or `uv.lock` with hashes<br>CI verifies lock integrity and matches pyproject constraints | Gate: lock drift check fails if deps change without lock update<br>Supply-chain test: install from lock only; run smoke tests | CI can build and install deterministically from lockfile. |
| I089 | Phase 0: Scaffolding and gates | Add canonical-json safety tests for journal/ledger payloads | Unit tests for canonical-json compliance across all event builders<br>Gate: `tools/gate_canon.py` runs in CI | Test: generate sample events from each plugin; validate canonical JSON<br>Gate: fail on floats/bytes/non-UTC timestamps | All emitted events pass canonical-json validation in CI. |
| I090 | Phase 0: Scaffolding and gates | Add concurrency tests for ledger/journal append correctness | Thread-safety tests for ledger/journal writers<br>Gate: `tools/gate_concurrency.py` | Test: multi-thread append; verify entry count and stable chain<br>Test: forced thread interleavings do not corrupt files | Ledger/journal remain valid under concurrent writes. |
| I091 | Phase 0: Scaffolding and gates | Add golden chain test: ledger verify + anchor verify | Golden test corpus for ledger chain + anchors<br>Gate: `tools/gate_ledger.py` verifies replay | Test: produce N entries; verify chain and anchor head deterministically<br>Test: tamper with one entry; verification fails | Verification reliably detects tampering and passes on untampered goldens. |
| I092 | Phase 0: Scaffolding and gates | Add performance regression tests (capture latency/memory/query latency) | Perf suite: capture tick latency, memory ceiling, query latency<br>Gate: `tools/gate_perf.py` with thresholds | Bench: sustained capture at configured fps; assert bounded RAM<br>Bench: query over N records completes under budget | Perf gate is deterministic and prevents accidental regressions. |
| I093 | Phase 0: Scaffolding and gates | Add security regression tests (DPAPI fail-closed, network guard, no raw egress) | Security test suite run in CI<br>Gate: `tools/gate_security.py` | Test: DPAPI failure with encryption_required causes hard failure<br>Test: kernel process cannot open network sockets<br>Test: unsanitized egress blocked unless dangerous_ops enabled | Security regressions are caught automatically. |
| I094 | Phase 0: Scaffolding and gates | Static analysis: ruff + typing + vuln scan | Static analysis in CI (lint, type checks, dependency scan) | CI fails on new lint/type errors<br>CI fails on high-severity dependency vulnerabilities (policy-defined) | Main branch stays lint/type clean; security issues surfaced early. |
| I095 | Phase 0: Scaffolding and gates | Doctor validates locks, storage, anchors, and network policy | `autocapture doctor` (new) + `tools/gate_doctor.py` | Test: doctor detects missing lockfile, plugin hash mismatch, bad perms<br>Test: doctor output is stable (snapshot test) | Doctor reliably detects misconfigurations before runtime failures. |
| I126 | Phase 0: Scaffolding and gates | Make sha256_directory path sorting deterministic across OSes | `autocapture_nx/kernel/hashing.py` sha256_directory ordering<br>Used by plugin hashing + contract lock hashing | Test: same directory hashed on Windows/Linux yields identical digest<br>Gate: plugin lock update is deterministic on same content | Hashing is deterministic across OS and filesystem orderings. |
| I001 | Phase 1: Correctness + immutability blockers | Eliminate floats from journal/ledger payloads | EventBuilder + canonical_json encoder<br>All plugin event payloads | Gate-CANON: reject floats/bytes; unit tests for all event types<br>Test: capture disk-pressure event emits integer bytes, not floats | No runtime path emits floats into canonical JSON; CI enforces. |
| I002 | Phase 1: Correctness + immutability blockers | Make backpressure actually affect capture rate | capture_windows pipeline timing loop<br>runtime governor → capture fps/quality controller | Test: backpressure changes fps target and measured interval responds<br>Perf: capture tick p95 stays within budget under disk pressure | When disk/queue pressure rises, capture rate drops within 1 second. |
| I003 | Phase 1: Correctness + immutability blockers | Stop buffering whole segments in RAM; stream segments | capture_windows: segment packer/writer<br>media store write path supports streaming | Perf: sustained capture uses bounded RAM (ceiling configured)<br>Test: segments written continuously without OOM on large resolutions | Capture can run indefinitely without unbounded memory growth. |
| I004 | Phase 1: Correctness + immutability blockers | Do not write to storage from realtime audio callback | audio_windows: callback produces into queue only<br>audio writer thread performs storage IO | Test: callback path performs no disk IO (mock store asserts not called)<br>Perf: audio capture has no xruns under load (best-effort check) | Audio callback is realtime-safe and never blocks on IO. |
| I005 | Phase 1: Correctness + immutability blockers | Stop mutating primary evidence metadata during query | query pipeline (`autocapture_nx/kernel/query.py`)<br>metadata store API: forbid overwrites for primary evidence | Gate-IMMUT: detect `put_replace` on evidence types<br>Test: extraction creates `derived.*` record; parent unchanged (hash stable) | No query path mutates primary evidence; citations remain stable over time. |
| I006 | Phase 1: Correctness + immutability blockers | Introduce globally unique run/session identifier; prefix all record IDs | kernel run manager (new): run_id creation and propagation<br>all plugins include run_id in IDs and events | Test: two runs produce non-colliding IDs even with same sequences<br>Gate: lint rule forbids bare `segment_0` style IDs in plugins | Replaying a second run never overwrites or collides with prior data. |
| I007 | Phase 1: Correctness + immutability blockers | Make ledger writing thread-safe | ledger writer: single-writer lock + atomic append semantics | Concurrency tests (Phase 0 I090) validate chain under multi-threading | Ledger chain remains valid under concurrent plugin writes. |
| I008 | Phase 1: Correctness + immutability blockers | Make journal writing thread-safe; centralize sequences | journal writer centralizes sequencing and timestamps<br>plugins stop maintaining their own `seq += 1` counters | Test: concurrent writes produce strictly increasing per-stream sequence<br>Snapshot: journal schema stable and contains run_id | All plugins emit events through EventBuilder+JournalWriter with stable schema. |
| I009 | Phase 1: Correctness + immutability blockers | Fail closed if DPAPI protection fails when encryption_required | keyring DPAPI unprotect path<br>config: `storage.encryption_required` hard requirement | Security test: DPAPI fail leads to startup failure when encryption_required<br>Doctor reports actionable remediation (recreate vault, permissions, etc.) | No path silently falls back to unprotected key bytes when encryption_required. |
| I010 | Phase 1: Correctness + immutability blockers | Sort all store keys deterministically | store implementations: keys() ordering and iteration | Test: repeated `keys()` calls return identical order<br>Gate: retrieval is deterministic given identical data | Iteration order is stable and deterministic across runs. |
| I011 | Phase 1: Correctness + immutability blockers | Use monotonic clocks for segment duration | capture timing: use monotonic for intervals; UTC timestamps for provenance | Test: system clock changes do not break segment scheduling | Capture schedule is robust to wall-clock adjustments. |
| I012 | Phase 1: Correctness + immutability blockers | Align default config with implemented capture backend | config/default.json + config schema<br>capture plugin selection logic | Doctor warns if config selects unsupported backend<br>Test: default config runs capture without unsupported backend errors | Out-of-the-box config matches real implementation. |
| I013 | Phase 1: Correctness + immutability blockers | Remove hard-coded model paths; config-driven + portable | ML plugins: model cache/weights paths in config<br>doctor validates paths exist or can be downloaded | Test: no absolute host-specific paths in repo<br>Doctor: warns when model missing; offers download command | Repo works on fresh machine without editing hard-coded paths. |
| I014 | Phase 1: Correctness + immutability blockers | Enforce plugin compat.requires_kernel / schema versions | plugin loader enforces compat fields at load time<br>contracts define schema versions and kernel ABI | Test: incompatible plugin is refused with clear error<br>Doctor: lists plugin compat mismatches | No plugin can run against incompatible kernel/schema without explicit override. |
| I015 | Phase 1: Correctness + immutability blockers | Verify contract lock at boot/doctor | contract lock verification at boot and doctor<br>`contracts/lock.json` must match hashes of contract files | Gate: contract lock verify in CI and on startup<br>Test: modifying contract file without lock update fails | Contracts cannot drift silently; every drift is detected. |
| I096 | Phase 1: Correctness + immutability blockers | Fail loud on decrypt errors when encryption_required | Encrypted stores: decrypt error handling under encryption_required | Test: corrupted ciphertext causes explicit error, not silent default<br>Doctor: can detect corruption and suggest recovery steps | Corruption is surfaced deterministically and does not produce false data. |
| I097 | Phase 1: Correctness + immutability blockers | Add record type fields everywhere | All event/record schemas include `record_type` or `event_type` | Schema tests ensure `record_type` present for all stored records | Every stored record can be typed without inspecting arbitrary fields. |
| I098 | Phase 1: Correctness + immutability blockers | Add unified EventBuilder helper | New `autocapture_nx/kernel/event_builder.py`<br>All plugins use EventBuilder for journal/ledger payloads | Gate: forbid direct JournalWriter/LedgerWriter calls outside EventBuilder<br>Test: EventBuilder outputs canonical-json-safe payloads | Plugins emit consistent, validated events via a single API. |
| I099 | Phase 1: Correctness + immutability blockers | Stamp every journal event with run_id | JournalWriter automatically inserts run_id (from run manager) | Test: all journal events include run_id | Journal is always partitionable by run_id. |
| I100 | Phase 1: Correctness + immutability blockers | Cache policy snapshot hashing per run | policy snapshot hashing computed once per run and reused<br>capture records store `policy_hash` only | Perf test: capture loop no longer recomputes policy hash per segment<br>Test: policy hash stable for a run and changes when config changes | Policy hashing overhead is removed from hot path. |
| I123 | Phase 1: Correctness + immutability blockers | Write kernel boot ledger entry system.start | kernel boot sequence writes `system.start` ledger entry | Golden ledger test includes start entry<br>Doctor verifies presence of start entry for completed runs | Every run has a verifiable origin entry in the ledger. |
| I124 | Phase 1: Correctness + immutability blockers | Write kernel shutdown ledger entry system.stop | kernel shutdown writes `system.stop` ledger entry with final head hash | Test: graceful shutdown emits stop entry | Every clean run has an explicit termination entry. |
| I125 | Phase 1: Correctness + immutability blockers | Write crash ledger entry on next startup | startup recovery writes `system.crash_detected` if prior run missing stop | Test: simulate crash (no stop) then restart emits crash entry | Crashes are recorded and do not silently break provenance. |
| I016 | Phase 2: Capture pipeline refactor | Split capture into grab → encode/pack → encrypt/write pipeline | capture_windows plugin refactor into 3-stage pipeline<br>interfaces: grabber, encoder/packer, encrypted writer | Perf: capture tick p95 within budget while pipeline backlog grows<br>Test: pipeline stages can be independently throttled/cancelled | Capture remains stable under load and isolates slow disk/encode paths. |
| I017 | Phase 2: Capture pipeline refactor | Bounded queues with explicit drop policies | queue primitives in capture/audio/input pipelines<br>drop policy documented and recorded | Test: queue never grows beyond configured max<br>Test: drops are recorded in metadata and journal | System remains bounded; any fidelity loss is explicit and auditable. |
| I018 | Phase 2: Capture pipeline refactor | Replace zip-of-JPEG with real video container for primary artifact | media container format for primary capture evidence<br>segment format versioning | Test: segment decode/extract works on all supported OS targets<br>Test: container metadata timestamps align with recorded ts_start/end | Primary capture evidence is efficient to store and seek deterministically. |
| I019 | Phase 2: Capture pipeline refactor | Add GPU-accelerated capture/encode backend (NVENC/DD) | New capture backend plugin(s): Desktop Duplication + NVENC<br>Config: `capture.backend` selects backend | Perf: CPU usage drops vs mss baseline at target resolution/fps<br>Security: subprocess sandbox for encoder if using external binaries | On capable GPUs, capture runs with minimal CPU while maintaining fidelity. |
| I020 | Phase 2: Capture pipeline refactor | Record segment start/end timestamps | capture metadata schema includes `ts_start_utc` and `ts_end_utc` | Test: segments always have valid start/end with end >= start | Every segment is time-bounded and usable for timeline queries. |
| I021 | Phase 2: Capture pipeline refactor | Record capture parameters per segment | capture metadata includes capture parameters and achieved metrics | Schema test ensures required capture params exist | Segments are self-describing for reproducibility and debugging. |
| I022 | Phase 2: Capture pipeline refactor | Correlate frames with active window via synchronized timeline | window timeline store + correlation logic in retrieval | Test: given window-change events, frame-to-window mapping is correct | Answers can cite which window/app a frame belonged to at a time. |
| I023 | Phase 2: Capture pipeline refactor | Add cursor/input correlation timeline references | input events and cursor state exposed as timelines<br>correlation references stored in derived artifacts | Test: correlation graph includes references from text/citation to input bursts | Investigations can align what was seen with what was done (time-synced). |
| I024 | Phase 2: Capture pipeline refactor | Disk pressure degrades capture quality before stopping | disk pressure controller in capture pipeline<br>policy thresholds in config | Test: under simulated low disk, capture degrades (fps/quality) before stop<br>Journal: emits `disk.pressure` and `capture.degrade` events | Capture fails gracefully and predictably under storage pressure. |
| I025 | Phase 2: Capture pipeline refactor | Atomic segment writes (temp + os.replace) | all store writes are atomic (temp + replace)<br>manifest and seal records only written after success | Test: crash mid-write does not produce partially visible evidence<br>Recovery scanner (I104) reconciles temp artifacts safely | No partial evidence becomes 'valid' without explicit seal. |
| I105 | Phase 2: Capture pipeline refactor | If keeping zips, use ZIP_STORED for JPEG frames | legacy zip segment path (if retained) uses ZIP_STORED | Perf: segment packing CPU drops vs deflate for JPEG frames | Legacy zip path is less CPU-expensive and remains correct. |
| I106 | Phase 2: Capture pipeline refactor | If keeping zips, stream ZipFile writes to a real file | legacy zip writer streams to file (no BytesIO) | Perf: no large in-memory segment buffers<br>Test: zip is valid and contains expected files | Zip mode does not require segment-sized RAM buffers. |
| I107 | Phase 2: Capture pipeline refactor | Batch input events to reduce write overhead | input_windows batches events by time window (e.g., 100–250ms)<br>JournalWriter supports batch append | Perf: input plugin reduces write rate under heavy input<br>Test: event ordering within batch preserved and timestamped | Input capture is scalable without overwhelming IO. |
| I109 | Phase 2: Capture pipeline refactor | Add WASAPI loopback option for system audio capture | audio_windows supports WASAPI loopback capture mode<br>Config selects microphone vs loopback | Test: device enumeration deterministic; loopback selection works on CI mocks | System audio can be captured as a first-class source when enabled. |
| I110 | Phase 2: Capture pipeline refactor | Store audio as PCM/FLAC/Opus derived artifact | audio stored as derived artifact blocks with clear encoding | Test: audio roundtrip decode yields expected sample count | Audio artifacts are decodable, time-aligned, and provenance-tracked. |
| I111 | Phase 2: Capture pipeline refactor | Normalize active window process paths (device → drive paths) | window metadata plugin normalizes process paths | Test: device path conversion deterministic given known mappings | Process paths are searchable and consistent across sessions. |
| I112 | Phase 2: Capture pipeline refactor | Capture window.rect and monitor mapping | window metadata schema includes rect and monitor mapping | Test: rect fields present and valid; monitor id matches layout snapshot | Window location can be correlated with capture frames deterministically. |
| I113 | Phase 2: Capture pipeline refactor | Optional cursor position+shape capture | optional cursor capture plugin or extension to capture_windows | Test: cursor capture disabled by default; when enabled, schema valid<br>Security: ensure cursor capture does not leak privileged info beyond local store | Cursor timeline is accurate when enabled and has bounded overhead. |
| I026 | Phase 3: Storage scaling + durability | Default to SQLCipher for metadata when available | storage backend selection: prefer SQLCipher for metadata<br>config: `storage.metadata_backend=sqlcipher|encrypted_fs` | Test: metadata queries faster than directory scan at N records<br>Security: DB file encrypted and unreadable without key | Metadata operations scale; encryption remains enforced. |
| I027 | Phase 3: Storage scaling + durability | Add DB indexes on ts_utc, record_type, run_id | SQLCipher schema: indexes on `ts_utc`, `record_type`, `run_id` | EXPLAIN-based test: queries use indexes for common patterns | Time-bounded queries are sub-linear and predictable. |
| I028 | Phase 3: Storage scaling + durability | Store media in binary encrypted format (not base64 JSON) | media store format: binary encrypted files with versioned header | Test: media blobs are not valid JSON and have expected magic/version<br>Test: decrypt+verify hash roundtrip works | Media storage is efficient and unambiguous; supports streaming. |
| I029 | Phase 3: Storage scaling + durability | Stream encryption (avoid whole-segment in memory) | media encryption writer supports streaming/chunking | Perf: writing large segments does not allocate segment-sized RAM<br>Test: chunk boundaries validate and reject tampering | Large artifacts are written/read with bounded memory and strong integrity. |
| I030 | Phase 3: Storage scaling + durability | Immutability/versioning in stores (put_new vs put_replace) | store APIs: `put_new()` for immutable; `put_replace()` only for caches<br>kernel enforces type-based mutability rules | Gate-IMMUT: evidence types cannot call replace; tests enforce | Evidence immutability is enforced by API, not convention. |
| I031 | Phase 3: Storage scaling + durability | Make record ID encoding reversible (no lossy mapping) | record ID encoding/locator encoding is reversible | Test: encode→decode roundtrip yields same ID for all legal IDs | IDs remain canonical and collision-free while being filesystem-safe. |
| I032 | Phase 3: Storage scaling + durability | Shard media/metadata directories by date/run | storage layout: shard by run/date to limit directory sizes | Perf test: listing/iterating keys remains fast at large scale | Storage remains performant as record count grows. |
| I033 | Phase 3: Storage scaling + durability | Add per-run storage manifest records | per-run manifest records (Pattern A) stored and ledgered | Test: manifest exists for each run and includes expected hashes | Each run is reproducible/auditable from a single manifest. |
| I034 | Phase 3: Storage scaling + durability | Configurable fsync policy (critical vs bulk) | fsync policy config applied in writers | Crash test: critical records survive; bulk media may lag but seals prevent inconsistency | Durability is explicit, configurable, and does not compromise provenance. |
| I101 | Phase 3: Storage scaling + durability | Add content_hash to metadata for every media put | media store returns content_hash; metadata stores it for every blob | Test: content_hash present and matches recomputed hash after decrypt | Evidence can be verified end-to-end by hash. |
| I102 | Phase 3: Storage scaling + durability | Track partial failures explicitly in journal/ledger | all failures emit typed journal/ledger events | Test: injected failures produce explicit failure records | Failures are visible, auditable, and do not silently corrupt state. |
| I103 | Phase 3: Storage scaling + durability | Add segment sealing ledger entry after successful write | segment lifecycle includes explicit `segment.sealed` ledger entry | Test: sealed only after media+metadata committed and hashes known | A segment is only considered valid if sealed. |
| I104 | Phase 3: Storage scaling + durability | Add startup recovery scanner to reconcile stores | startup recovery scanner reconciles temp/partial artifacts<br>writes recovery actions to ledger/journal | Crash simulation: partial writes detected and repaired/quarantined | System self-heals from crashes without silently losing provenance. |
| I108 | Phase 3: Storage scaling + durability | Add compact binary input log (derived) + JSON summary | input pipeline writes compact binary derived log + JSON summary records | Test: binary log roundtrip decode; JSON summary matches counts/time range | Input data scales without losing queryability or provenance. |
| I128 | Phase 3: Storage scaling + durability | Tooling to migrate data_dir safely (copy+verify, no delete) | CLI/API command: `storage.migrate` copy+verify strategy | Test: migrate copies all evidence and manifests; verification passes | Users can move data safely without data loss or provenance breakage. |
| I129 | Phase 3: Storage scaling + durability | Disk usage forecasting (days remaining) + alerts | telemetry + alerts: disk usage trends and days remaining estimate | Test: forecasting produces deterministic output for fixed input series | System warns before disk exhaustion and informs mitigation choices. |
| I130 | Phase 3: Storage scaling + durability | Storage compaction for derived artifacts only | compaction applies only to derived artifacts and rebuildable indexes | Gate-IMMUT: compaction never touches primary evidence<br>Test: compaction reduces size; citations still resolve | Storage is optimized without compromising evidence immutability or citations. |
| I035 | Phase 4: Retrieval + provenance + citations | Replace full-scan query with tiered indexed retrieval | retrieval pipeline plugin replaces full-scan approach<br>SQLCipher/FTS + optional embeddings | Perf: query latency improves at N records vs full scan baseline<br>Accuracy: golden queries return expected evidence set deterministically | Queries scale without full scans and remain deterministic. |
| I036 | Phase 4: Retrieval + provenance + citations | Deterministic retrieval ordering (stable sort keys) | retrieval sorts by stable keys: time, record_type, evidence_id | Test: same dataset yields identical ranked output across runs | Retrieval ordering is stable and reproducible. |
| I037 | Phase 4: Retrieval + provenance + citations | Candidate-first extraction (retrieve then extract) | extraction planner: select candidates then extract | Perf: extraction work bounded to top-K candidates<br>Accuracy: extraction uses explicit time/span constraints | Extraction cost is bounded and targeted; no random scans. |
| I038 | Phase 4: Retrieval + provenance + citations | Derived artifact records for OCR/VLM outputs | schema: `derived.text.*` records<br>store APIs enforce immutability (I30) | Test: derived text record includes parent reference and span_ref<br>Test: model identity fields present and hashed | All extraction outputs are provenance-tracked derived artifacts. |
| I039 | Phase 4: Retrieval + provenance + citations | Ledger query executions (inputs/outputs) | query execution writes ledger entry: inputs + retrieval plan + outputs | Golden: query ledger entry reproducible for fixed corpus | Every answer can be tied to a ledgered query execution record. |
| I040 | Phase 4: Retrieval + provenance + citations | Ledger extraction operations (inputs/outputs) | extraction writes ledger entry linking inputs/outputs | Test: derived artifacts have corresponding ledger derivation entries | Derivations are verifiable and reconstructable. |
| I041 | Phase 4: Retrieval + provenance + citations | Citations point to immutable evidence IDs + spans | citation schema uses evidence_id + span_ref + hashes | Schema test: citations cannot be created without required fields | Citations are independently verifiable pointers, not best-effort strings. |
| I042 | Phase 4: Retrieval + provenance + citations | Citation resolver validates hashes/anchors/spans | citation resolver service + CLI validates evidence and provenance | Test: resolver detects tampering and missing spans | Every citation displayed in UI can be validated locally on demand. |
| I043 | Phase 4: Retrieval + provenance + citations | Fail closed if citations do not resolve | answer pipeline refuses to return `state=ok` if citations unresolved | Golden tests: unresolved citations produce `state=no_evidence` or `state=partial` | System never asserts unsupported answers; failure modes are explicit. |
| I065 | Phase 4: Retrieval + provenance + citations | Define canonical evidence model (EvidenceObject) | contracts: evidence schema and versioning<br>EventBuilder enforces EvidenceObject fields | Schema tests cover all evidence types and require minimal fields | Evidence model is consistent and contract-checked. |
| I066 | Phase 4: Retrieval + provenance + citations | Hash everything that matters (media/metadata/derived) | hashing: media plaintext SHA-256; metadata canonical-json hash | Verify: recomputed hashes match stored hashes for sample corpus | Every evidence/derived object is hash-addressable and verifiable. |
| I067 | Phase 4: Retrieval + provenance + citations | Ledger every state transition | ledger coverage expanded across all plugins and transitions | Gate: required event types appear for each run (start, evidence writes, stop/crash) | Ledger provides complete provenance coverage for the system's actions. |
| I068 | Phase 4: Retrieval + provenance + citations | Anchor on schedule (N entries or M minutes) | anchor plugin anchors ledger head on schedule | Test: anchors created at configured cadence; verification passes | Ledger heads are periodically sealed for tamper evidence. |
| I069 | Phase 4: Retrieval + provenance + citations | Immutable per-run manifest (config+locks+versions) | run manifest evidence record written early and finalized at end | Test: manifest includes config/plugin/contracts hashes + versions | A single manifest summarizes and identifies the full run context. |
| I070 | Phase 4: Retrieval + provenance + citations | Citation objects carry verifiable pointers | citation schema: include evidence hash + ledger/anchor refs | Resolver rejects citations missing required verification fields | Citations are self-contained verification units. |
| I071 | Phase 4: Retrieval + provenance + citations | Citation resolver CLI/API | CLI/API endpoints: `verify citation`, `resolve citation` | Golden: resolver output stable and correct for known citations | Users can verify citations with one command. |
| I072 | Phase 4: Retrieval + provenance + citations | Metadata immutable by default; derived never overwrites | store mutability policies enforced per record_type (I30) | Gate-IMMUT catches any overwrite on evidence/derived records | Immutability is enforced uniformly across the codebase. |
| I073 | Phase 4: Retrieval + provenance + citations | Persist derivation graphs (parent→child links) | derivation graph store and schema<br>ledger entries reference derivation edges | Test: derived artifacts create derivation edge to parent | Any derived output can be traced back to its precise evidence parents. |
| I074 | Phase 4: Retrieval + provenance + citations | Record model identity for ML outputs | model identity schema and recording in derived artifacts<br>model manager records weight digests (Phase 6) | Test: derived artifacts contain model_name + model_digest + params | ML outputs are reproducible and attributable to exact model artifacts. |
| I075 | Phase 4: Retrieval + provenance + citations | Deterministic text normalization before hashing | text normalization function used before hashing and indexing | Test: normalization is deterministic and stable on sample inputs | Text hashes and indexes are stable and reproducible. |
| I076 | Phase 4: Retrieval + provenance + citations | Proof bundles export (evidence + ledger slice + anchors) | proof bundle exporter in UX facade | Test: exported bundle verifies on a clean machine without network | Bundles are self-contained, verifiable, and suitable for audit/sharing (sanitized). |
| I077 | Phase 4: Retrieval + provenance + citations | Replay mode validates citations without model calls | replay tool validates retrieval/citations without model calls | Golden: replay reproduces expected citations and verification results | Citations can be validated deterministically offline. |
| I118 | Phase 4: Retrieval + provenance + citations | Index versioning for retrieval reproducibility | indexes record version/digest in manifest and query ledger entries | Test: query includes index version refs; rebuild increments version | Answers can identify which index snapshot they used. |
| I127 | Phase 4: Retrieval + provenance + citations | Record python/OS/package versions into run manifest | run manifest captures environment versions | Test: manifest contains python version, OS build, package versions list | Runs are attributable to an environment fingerprint for debugging and audits. |
| I044 | Phase 5: Scheduler/governor | Real scheduler plugin gates heavy work on user activity | new scheduler plugin (builtin): job admission based on activity<br>governor API used by all heavy workers | Test: ACTIVE mode prevents OCR/VLM/embeddings/indexing jobs from running<br>Test: IDLE mode allows bounded enrichment and records budgets in journal | No heavy processing occurs while user active; enrichment happens predictably when allowed. |
| I045 | Phase 5: Scheduler/governor | Input tracker exposes activity signals (not only journal) | input_windows exposes aggregated activity signal channel to governor | Test: simulated input produces immediate ACTIVE signal<br>Test: inactivity decays to IDLE after configured timeout | Scheduler has an authoritative, low-latency activity signal. |
| I046 | Phase 5: Scheduler/governor | Capture emits telemetry (queues, drops, lag, CPU) | capture pipeline emits telemetry events and exposes metrics endpoint | Test: telemetry includes queue depths, drops, lag, CPU<br>Websocket (I83) streams telemetry with stable schema | Performance issues are observable without instrumenting code manually. |
| I047 | Phase 5: Scheduler/governor | Governor outputs feed backpressure and job admission | governor decisions affect capture rate/quality and enrichment admission | Integration test: governor changes lead to fps/quality changes within bounded time | System responds quickly and deterministically to activity and pressure changes. |
| I048 | Phase 5: Scheduler/governor | Immediate ramp down on user input (cancel/deprioritize heavy jobs) | scheduler cancels/deprioritizes heavy jobs on new input events | Test: user input interrupts ongoing enrichment within timeout budget | User interaction immediately restores invisibility by halting heavy work. |
| I116 | Phase 5: Scheduler/governor | Model execution budgets per idle window | scheduler budgets for model execution (CPU/GPU time, tokens, batch sizes) | Test: budgets enforced; jobs stop/continue across idle windows without violating budget | ML workloads are bounded and cannot starve the system. |
| I117 | Phase 5: Scheduler/governor | Preemption/chunking for long jobs | all long jobs implemented as chunked work units with checkpoints | Test: job can be paused/resumed without redoing completed work | Heavy processing is preemptible and resumes deterministically. |
| I049 | Phase 6: Security + egress hardening | Egress gateway must be subprocess-hosted; kernel network-denied | process model: kernel has network denied; egress plugin runs in subprocess<br>network guard applied at OS/process boundary where possible | Security test: kernel cannot reach network even if code tries<br>Test: egress plugin can reach allowlisted endpoints only | Network access is centralized, auditable, and policy-controlled. |
| I050 | Phase 6: Security + egress hardening | Minimize inproc_allowlist; prefer subprocess hosting | plugin registry: inproc_allowlist minimized; default subprocess | Gate: fail if new inproc plugin added without security justification entry | Kernel attack surface reduced while keeping capture performance. |
| I051 | Phase 6: Security + egress hardening | Capability bridging for subprocess plugins (real capability plumbing) | subprocess host runner provides real capability access (not None) | Test: subprocess plugin receives only declared capabilities and can operate | Subprocess plugins are first-class and do not require inproc hosting to function. |
| I052 | Phase 6: Security + egress hardening | Enforce least privilege per plugin manifest | plugin manifest declares required capabilities; loader enforces deny-by-default | Test: plugin without declared capability cannot access it | Least privilege is enforced mechanically. |
| I053 | Phase 6: Security + egress hardening | Enforce filesystem permission policy declared by plugins | filesystem sandboxing for subprocess plugins<br>manifest-declared read/readwrite paths enforced | Test: plugin cannot read outside allowed roots (integration) | Plugins cannot exfiltrate or tamper with files beyond their declared scope. |
| I054 | Phase 6: Security + egress hardening | Strengthen Windows job object restrictions (limits) | Windows job object: CPU/memory limits, kill-on-close, child restrictions | Test: runaway plugin is terminated and reported | Untrusted/hung plugins are contained without harming system stability. |
| I055 | Phase 6: Security + egress hardening | Sanitize subprocess env; pin caches; disable proxies | subprocess environment sanitizer in host | Test: proxy env vars removed; cache dirs pinned | Plugin behavior is deterministic and not affected by ambient environment. |
| I056 | Phase 6: Security + egress hardening | Plugin RPC timeouts and watchdogs | plugin RPC layer: timeouts + watchdog to restart hung plugins | Test: hung plugin call times out and system recovers without deadlock | Plugin failures do not stall capture or UI. |
| I057 | Phase 6: Security + egress hardening | Max message size limits in plugin RPC protocol | plugin RPC protocol enforces max message sizes and streaming | Test: oversized message rejected; chunked streaming used for large blobs | IPC is resilient and cannot be abused for memory exhaustion. |
| I058 | Phase 6: Security + egress hardening | Harden hashing against symlinks / filesystem nondeterminism | hashing policy: symlink handling defined and enforced | Test: symlinks in plugin root are rejected or hashed deterministically | Hashing cannot be bypassed via filesystem tricks. |
| I059 | Phase 6: Security + egress hardening | Secure vault file permissions (Windows ACLs) | vault and data dirs created with restrictive ACLs on Windows | Test: created files are not world-readable (platform-dependent assertions) | Secrets and encrypted stores are protected by OS permissions. |
| I060 | Phase 6: Security + egress hardening | Separate keys by purpose (metadata/media/tokenization/anchor) | key manager defines separate keys per purpose + key ids | Test: rotating one key does not break others; derived artifacts remain readable as policy dictates | Key separation limits blast radius and supports safe rotation. |
| I061 | Phase 6: Security + egress hardening | Anchor signing (HMAC/signature) with separate key domain | anchor plugin uses HMAC/signature over ledger head | Test: anchor verify fails if anchor modified or wrong key used | Anchors provide tamper evidence independent of ledger storage. |
| I062 | Phase 6: Security + egress hardening | Add verify commands (ledger/anchors/evidence) | verify CLI/API: `verify ledger`, `verify anchors`, `verify evidence` | Golden verification suite passes; tamper cases fail | Users can validate integrity with deterministic tooling. |
| I063 | Phase 6: Security + egress hardening | Audit security events in ledger (key rotations, lock updates, config) | ledger event types for security-relevant operations | Test: key rotation and lock updates emit ledger entries | Security posture changes are auditable and tamper-evident. |
| I064 | Phase 6: Security + egress hardening | Dependency pinning + hash checking (supply chain) | runtime dependency verification and build-time pinning | CI verifies dependency hashes; runtime doctor reports mismatches | Supply chain drift is detected and controlled. |
| I119 | Phase 6: Security + egress hardening | Persist entity-tokenizer key id/version; version tokenization | tokenizer plugin stores key id/version used for each tokenization | Test: tokenization output is stable under same key id; rotation yields new version | Tokenization is reproducible and rotation-aware. |
| I120 | Phase 6: Security + egress hardening | Ledger sanitized egress packets (hash + schema version) | egress gateway writes ledger entry for each sanitized outbound packet | Test: egress attempt emits `egress.packet` ledger entry with hash + schema version | Outbound actions are fully auditable without storing raw sensitive payloads. |
| I078 | Phase 7: FastAPI UX facade + Web Console | FastAPI UX facade as canonical interface | new FastAPI app provides canonical UX facade endpoints<br>kernel exposes only internal APIs; UI/CLI call facade | API contract tests: endpoints stable and validated against schemas<br>Security: binds to localhost by default; requires auth token (I82) | All user interactions route through a single, testable facade. |
| I079 | Phase 7: FastAPI UX facade + Web Console | CLI parity: CLI calls shared UX facade functions | CLI uses shared UX facade functions (no direct kernel calls) | Test: CLI commands produce identical results as API endpoints | UI/CLI parity is enforced and drift is prevented. |
| I080 | Phase 7: FastAPI UX facade + Web Console | Web Console UI (status/timeline/query/proof/plugins/keys) | Web Console served by FastAPI (or static assets) as sole maintained UI | UI smoke tests: load pages and call API endpoints (headless)<br>API snapshot tests for UI-critical views | Web UI covers all critical workflows without requiring CLI. |
| I081 | Phase 7: FastAPI UX facade + Web Console | Alerts panel driven by journal events | alerts derived from journal/telemetry streams | Test: disk pressure and capture drops appear as alerts | Operational problems are visible immediately with actionable context. |
| I082 | Phase 7: FastAPI UX facade + Web Console | Local-only auth boundary (bind localhost + token) | API auth middleware: token-based auth; localhost binding by default | Security test: state-changing endpoints require auth token | Local API is not accidentally exposed or writable without authorization. |
| I083 | Phase 7: FastAPI UX facade + Web Console | Websocket for live telemetry | websocket endpoint streams telemetry and job status | Test: websocket schema stable and rate-limited | UI can display live status without polling overhead. |
| I121 | Phase 7: FastAPI UX facade + Web Console | Egress approval workflow in UI | UI + facade implement explicit approval flow for any outbound egress | Test: egress blocked without approval; approved egress logs approval id + packet hash (I120) | Outbound actions are user-controlled, auditable, and sanitized by default. |
| I114 | Phase 8: Optional expansion plugins | Clipboard capture plugin (local-only, append-only) | new clipboard capture plugin (subprocess-hosted by default) | Test: disabled by default; when enabled, records are append-only and ledgered<br>Security: redaction policy for sensitive clipboard types (optional) is explicit | Clipboard history is captured locally with provenance and controlled overhead. |
| I115 | Phase 8: Optional expansion plugins | File activity capture plugin (USN journal / watcher) | new file activity plugin (Windows USN journal or watcher) | Test: disabled by default; when enabled, events are time-ordered and searchable | File activity timeline can be correlated with other evidence by time. |
| I122 | Phase 8: Optional expansion plugins | Plugin hot-reload with hash verification and safe swap | plugin manager supports hot-reload for non-core plugins with hash verification | Test: hot-reload updates plugin only if lockfile updated and verified<br>Test: in-flight jobs are drained/cancelled safely on reload | Plugins can be updated without downtime while preserving security and determinism. |

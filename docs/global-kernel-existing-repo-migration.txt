# Migration Plan + Compliance Specs: Converting Existing Repos to the Global Kernel + WSL Plugin Model

**Goal:** Make existing repos “compliant” so they run under the same native Windows kernel + stable IPC contract, with plugins executed in WSL2 via the host.

---

## One round of questions (optional; assumptions provided)
1) Do existing repos already have **any** C# kernel/UI, or are they Python-only?  
2) How do existing repos launch WSL today (direct `wsl.exe`, bash scripts, Docker, etc.)?  
3) Are plugins currently isolated (separate processes) or in-proc modules?  
4) Do you already have a canonical “project folder” layout?  
5) Where do artifacts/logs live today?  
6) Any repo uses gRPC/HTTP already between Windows and WSL?  
7) Do you need to preserve backward compatibility with existing CLI entrypoints?  
8) Are there “must keep” UI frameworks (e.g., PyQt) that cannot be replaced immediately?

**Default assumptions (used if unanswered):**
- Many repos are Python-first with ad-hoc Windows UI or minimal UI; WSL invoked via scripts; artifacts scattered in repo folders; no existing gRPC boundary.

---

## 1) What “compliant” means (spec)
A repo is compliant when:
1) It contains `project.yaml` at its root (or the kernel can generate it).
2) All long-running compute is executed via **WSL Host** behind the stable protocol.
3) The repo’s Python components are packaged as one or more **plugins** with manifests + settings schemas.
4) Artifacts and retention are managed via the kernel/host policy (not ad-hoc deletion).
5) Jobs are submitted/cancelled/observed via the kernel job model.

---

## 2) Compliance levels (to support varying states)

### L0: Ad-hoc (Python scripts + manual WSL calls)
- No kernel; no plugin boundary; scripts run directly.

### L1: Partial kernel (some C# or Windows UI exists)
- Kernel exists but launches scripts directly; no stable protocol.

### L2: Plugin-ish (Python modules + config) but no isolation/contract
- Plugins exist conceptually; no manifest/schema; direct imports.

### L3: Contracted (has IPC) but incompatible contract
- Has gRPC/HTTP but needs mapping to standard protocol.

**Target:** L3-compliant with the standard protocol and manifests.

---

## 3) Migration strategy overview (min-risk)

### 3.1 First milestone: wrap existing behavior without rewriting ML
- Introduce **WSL Host** and a **Legacy Adapter Plugin**:
  - Plugin that calls existing scripts/modules with minimal changes.
- Introduce `project.yaml` with path mappings and resource limits.
- Kernel can now run the repo under the standard job model.

### 3.2 Second milestone: convert scripts into real plugins
- Break legacy adapter into:
  - stable plugin entrypoint(s)
  - typed job inputs/outputs
  - settings schema
  - evidence bundling and artifact references

### 3.3 Third milestone: improve UX/performance
- Replace horrendous UI with schema-driven UI in kernel.
- Add scheduling/backpressure and cancellations.
- Add retention policy and purge jobs.

---

## 4) Required changes by repo state (playbooks)

### Playbook A (L0 → compliant): Python-only repo, no kernel
1) Add `project.yaml`.
2) Add `wsl_host/` deployment notes (or reference shared host).
3) Create `plugins/<repo_plugin>/`:
   - `plugin.toml`
   - `settings.schema.json`
   - plugin entrypoint that calls current pipeline functions
4) Replace direct execution with submitting jobs through kernel.
5) Standardize artifacts:
   - write outputs through artifact store interface
6) Add smoke tests:
   - run noop job + one real job

### Playbook B (L1 → compliant): has Windows UI/kernel but launches scripts directly
1) Keep existing UI temporarily.
2) Add protocol client layer:
   - switch “run pipeline” button to `SubmitJob` RPC flow
3) Add WSL host and legacy adapter plugin.
4) Gradually migrate UI settings into schema-driven settings:
   - export existing configs → generate schemas → adopt kernel UI
5) Deprecate direct `wsl.exe` invocations.

### Playbook C (L2 → compliant): plugin-like Python modules but no contract
1) Create plugin manifests for each module group.
2) Add worker isolation (process-per-job).
3) Add schemas for settings, validated by kernel and host.
4) Add job envelope and structured outputs.

### Playbook D (L3 → compliant): has IPC but incompatible
1) Write a protocol mapping layer (temporary):
   - map old endpoints → new `.proto` services
2) Migrate clients (kernel) to the standard contract.
3) Remove mapping layer once all callers move.

---

## 5) Blueprint: “Legacy Adapter Plugin” (critical for rapid adoption)

**Purpose:** call existing repo entrypoints without invasive refactor.

### 5.1 Adapter contract
- Job type: `legacy.run`
- Inputs:
  - `command` or `entrypoint` identifier (from an allowlist)
  - `args` (validated)
  - input artifact references
- Outputs:
  - stdout/stderr logs
  - exit code
  - output artifact references (declared paths)

### 5.2 Safety requirements
- Allowlist only (no arbitrary shell).
- Timeouts and cancellation supported.
- Artifacts must be written to scoped project paths only.

---

## 6) Config + settings migration

### 6.1 Map existing configs into schema-driven settings
- Create `settings.schema.json` capturing:
  - types, defaults, ranges
  - UI grouping
- Implement a `settings_migrate.py`:
  - reads old config
  - emits new schema-conformant config
  - produces a migration report

### 6.2 Backward compatibility window
- Maintain a `compat/` loader for N releases if needed:
  - reads old config
  - transforms into new config at runtime
  - warns in logs

---

## 7) Artifacts + retention compliance

### 7.1 Artifact normalization
- Every pipeline output becomes an `ArtifactRef`:
  - content-addressed
  - stored under `<project_root>/.artifacts/`
- Evidence bundle per job:
  - `inputs.json`, `outputs.json`, `config_hash.txt`

### 7.2 Retention policy enforcement
- Implement purge job type: `maintenance.purge`
- Kernel schedules purge per `project.yaml`
- Dry-run mode first; require explicit confirmation to enable destructive purge.

---

## 8) Regression detection + acceptance tests (must pass)
1) **Functional parity:** legacy pipeline output matches baseline (hash/semantic check).
2) **No leakage:** artifacts remain in project scope.
3) **Stability:** crash plugin worker does not crash host; crash host does not crash kernel.
4) **Performance safety:** UI remains responsive under load.
5) **Repeatability:** same inputs + config produce same outputs (within tolerated nondeterminism).

---

## 9) Implementation plan for modifying an existing repo (Codex-oriented)

### 9.1 Discovery checklist (Codex should fill)
- Current entrypoints:
  - CLI scripts, modules, UI buttons, scheduled tasks
- WSL invocation method:
  - `wsl.exe`, bash, python subprocess, etc.
- Output locations and formats:
  - files, sqlite, logs
- Long-running components:
  - watchers, capture loops, servers
- Existing config formats:
  - yaml/json/ini/env

### 9.2 Execution steps (ordered)
1) Add `project.yaml` and minimal defaults.
2) Add `plugins/legacy_adapter/` with manifest + schema.
3) Route current pipeline execution through adapter plugin in WSL host.
4) Add kernel integration:
   - project open
   - run one job
   - display progress + logs
5) Migrate artifacts into `.artifacts/` with stable references.
6) Convert one high-value pipeline from legacy adapter → native plugin API.
7) Add purge policy and schedule.
8) Add E2E smoke test to CI.

---

## DETERMINISM
DETERMINISM: PARTIAL (scoped)
- Deterministic: compliance definition, playbooks, adapter spec, migration sequence, acceptance tests.
- Non-deterministic: time/effort per repo and performance effects (requires repo inspection and measurement).

---

THREAD: 2026-02-01_windows-global-kernel-spec  
CHAT_ID: 20260201_1030_windows-global-kernel-spec  
TS: 2026-02-01 10:30 America/Denver
